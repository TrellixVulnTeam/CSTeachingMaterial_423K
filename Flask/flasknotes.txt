1. virtualenv "path of virtualenv"
2. will download and install all packages in this directory
3. cd the dir path
cd scripts
activate
4. the shell appears
5. pip install flask here
6. since not using pydev - python interpreter by typing python on the virtualenv prompt
7. following https://www.ntu.edu.sg/home/ehchua/programming/webprogramming/Python3_Flask.html


8. put the file in the scripts dir - since that is the path i gave

(bin) C:\Users\Gowri\Desktop\CS_classes\MaterialRepo\CSTeachingMaterial\Flask\mynewapp\bin\Scripts>python flaskapp.py
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)

9. run python flaskapp.py


-did not run on virtual environment

-passed data to javascript using jinja templating
-retrieve data from javascript and send it to python -using jquery or angularjs


NOTE: to have a virtual environment of Python - 2.7

http://docs.python-guide.org/en/latest/dev/virtualenvs/
-->virtualenv -p /usr/bin/python2.7 my_project

instead of bin/activate - scripts/activate
 
and use activate in windows - run activate in the Scripts directory


You are doing it on  2.7 to install scrapy -- did not work on python 3
 -----SCRAPY shell does not work

-->pip install pypiwin32

-- provides GPS coordinates
https://www.maps.ie/coordinates.html
https://www.latlong.net/
http://en.mygeoposition.com/

-->. now run scrapyshell

https://diyhacking.com/makerspaces/ 
either google or use some direct link like this


following this link - for scrapy 
https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/


Virtual directory was at C:\Users\Gowri
change directory to C:\Users\Gowri\Desktop\CS_classes\MaterialRepo\CSTeachingMaterial\Flask\fullstackapp

one level above the virtual environment and create a new directory - makerspacescrpaper

--> scrapy startproject makerspacescraper


cd to makerspacesscraper and execute

scrapy runspider makerspacescraper\spiders\makerspacebot.py -o table.csv


https://bl.ocks.org/mbostock/1353700
https://bl.ocks.org/sahirvsahirv/CSTeachingMaterial/blob/master/Map%20Colouring/data/index_greedy.html


-->Go to the virtual dir scripts folder and activate
--> cd to the makespace folder -> one level above the spider folder is
scrapy runspider makerspacescraper\spiders\makerspacebot.py -o table.csv
--->pick up xpath from the browser https://3583bytesready.net/2016/08/17/scraping-data-python-xpath/
---> and pycharm with virtualenv https://ohadp.com/pycharm-debug-inside-a-pip-in-a-virtual-env-97aa98eac77f
--> Go to configuration - of makerspacebot and set the script as run.py to enable debugging
--> in edit parameters -> makerspacebot sccript parameters enter"crawl makerspacebot"
https://stackoverflow.com/questions/21788939/how-to-use-pycharm-to-debug-scrapy-projects

1. Created a virtual env for 2.7
2. Installed scrapy on it
3. Wrote the spider
4. configuring pycharm for the virtualenv
5. Debugging the spider using pycharm
6. Configuring git
https://stackoverflow.com/questions/21788939/how-to-use-pycharm-to-debug-scrapy-projects

was creating a new instance of configuration- have to debug straight from the top
DEbugging done-

now scrape
->copy xpath 
-->ctrl F to get to find xpath

cd to spiders dir
scrapy crawl makerspacebot (the name given in name="")


Pipeline class
http://www.scrapingauthority.com/2016/09/19/scrapy-exporting-json-and-csv/

Differences
http://www.scrapingauthority.com/2016/10/03/scrapy-beautifulsoup-difference

yield: and put pipeline in pipelines.py
https://stackoverflow.com/questions/43922562/scrapy-how-to-use-items-in-spider-and-how-to-send-items-to-pipelines/43932615


TODO: 
use mongoDB
scrapy deamon - not as a deamon but start and stop on a falsk app using buttons
amazon login and scrape
deamon with flask
geojson api
cloud google
da and ml

scrapy not on cloud

have this data written to a json text - read and show geojson and current spot closest makerspace
and store current spots and see which area is most popular for makerspaces

enter current address and show nearest maker space to your current address

scrapy deamon

-- not using it, using flask api for scrapy and will run as a python deamon?
pip install scrapy deamon in the virtual env



install arachne
http://arachne.readthedocs.io/en/latest/

1. download the latest version of arachne on to the Scripts folder
2. and say pip install Arachne...


folder structure for flask
https://www.digitalocean.com/community/tutorials/how-to-structure-large-flask-applications

The directory structure of Arachne requires it to be in this folder
C:\Users\Gowri\Desktop\CS_classes\MaterialRepo\CSTeachingMaterial\Flask\fullstackapp\makerspacescraper\makerspacescraper


cd to C:\Users\Gowri\Desktop\CS_classes\MaterialRepo\CSTeachingMaterial\Flask\fullstackapp\makerspacescraper\makerspacescraper
and start

python Arachneflaskapp.py

- and that starts scrapy


run http://localhost:8080/ - scrapy app makerspacebot tries to run

Refer to demo
https://github.com/kirankoduru/arachne-demo

now call this link from another page


Flask help:
https://kirankoduru.github.io/python/sqlalchemy-pipeline-scrapy.html

-- sensors pick up information from AC's fans, heaters etc
--Predictor - future power consumption based on the data trends and hence not do
meetings that might need perhaps more than 50 when can manage only 50
--Track the power and smart meter system - what does it mean?
and how to track power?
--what are soft sensors? sensing current occupancy and smart door
--scan and wake up
--


1. read address from file
2. ask current address
3. see distance and plot
4. store all closest maker spaces and plot which is the most frequent


---3.4---- flask virtualenv 3.4
pip install flask-bootstrap
pip install flask-wtf for webforms


set the app's virtual environment to the new directory for debug

in File->Settings->Project Interpreter->project interpreter -> choose the 3.4 interpreter


--->pip install flask_bootstrap for basic form sytax
installed here
C:\Users\Gowri\Desktop\CS_classes\MaterialRepo\CSTeachingMaterial\Flask\fullstackapp\fullstackapp3.4\Lib\site-packages\flask_bootstrap



